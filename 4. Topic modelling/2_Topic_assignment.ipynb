{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "voluntary-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "arctic-fishing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.052*\"scienc\" + 0.039*\"scientist\" + 0.022*\"fact\" + 0.021*\"real\" + '\n",
      "  '0.015*\"datum\" + 0.014*\"theori\" + 0.012*\"wrong\" + 0.012*\"evid\" + '\n",
      "  '0.011*\"scientif\" + 0.011*\"true\" + 0.010*\"mani\" + 0.010*\"point\" + '\n",
      "  '0.010*\"claim\" + 0.010*\"proof\" + 0.009*\"alarmist\" + 0.009*\"truth\" + '\n",
      "  '0.009*\"report\" + 0.009*\"predict\" + 0.009*\"manmad\" + 0.008*\"research\"'),\n",
      " (1,\n",
      "  '0.071*\"year\" + 0.039*\"time\" + 0.039*\"earth\" + 0.036*\"man\" + 0.031*\"ice\" + '\n",
      "  '0.028*\"human\" + 0.019*\"natur\" + 0.018*\"planet\" + 0.017*\"age\" + 0.012*\"god\" '\n",
      "  '+ 0.011*\"life\" + 0.011*\"due\" + 0.010*\"sun\" + 0.010*\"part\" + 0.010*\"littl\" + '\n",
      "  '0.009*\"caus\" + 0.009*\"end\" + 0.008*\"long\" + 0.008*\"histori\" + 0.008*\"cycl\"'),\n",
      " (2,\n",
      "  '0.041*\"weather\" + 0.033*\"cold\" + 0.026*\"day\" + 0.019*\"snow\" + 0.017*\"today\" '\n",
      "  '+ 0.017*\"hot\" + 0.015*\"winter\" + 0.013*\"degre\" + 0.013*\"warm\" + '\n",
      "  '0.013*\"hurrican\" + 0.012*\"real\" + 0.012*\"water\" + 0.011*\"record\" + '\n",
      "  '0.011*\"high\" + 0.010*\"week\" + 0.009*\"air\" + 0.009*\"heat\" + 0.009*\"fuck\" + '\n",
      "  '0.008*\"temp\" + 0.008*\"summer\"'),\n",
      " (3,\n",
      "  '0.032*\"hoax\" + 0.031*\"trump\" + 0.028*\"gore\" + 0.025*\"fake\" + 0.012*\"stupid\" '\n",
      "  '+ 0.012*\"presid\" + 0.012*\"guy\" + 0.010*\"shit\" + 0.010*\"idiot\" + '\n",
      "  '0.010*\"great\" + 0.009*\"talk\" + 0.008*\"privat\" + 0.008*\"obama\" + 0.008*\"jet\" '\n",
      "  '+ 0.007*\"car\" + 0.007*\"hous\" + 0.007*\"video\" + 0.007*\"work\" + 0.006*\"joke\" '\n",
      "  '+ 0.006*\"white\"'),\n",
      " (4,\n",
      "  '0.029*\"liber\" + 0.016*\"polit\" + 0.015*\"democrat\" + 0.014*\"fire\" + '\n",
      "  '0.012*\"agenda\" + 0.011*\"medium\" + 0.011*\"issu\" + 0.010*\"dem\" + '\n",
      "  '0.009*\"thing\" + 0.009*\"california\" + 0.008*\"state\" + 0.008*\"leftist\" + '\n",
      "  '0.007*\"propaganda\" + 0.007*\"child\" + 0.007*\"kid\" + 0.007*\"blame\" + '\n",
      "  '0.007*\"gun\" + 0.007*\"conserv\" + 0.007*\"parti\" + 0.007*\"republican\"'),\n",
      " (5,\n",
      "  '0.037*\"world\" + 0.030*\"money\" + 0.025*\"big\" + 0.017*\"tax\" + 0.016*\"scam\" + '\n",
      "  '0.015*\"control\" + 0.013*\"govern\" + 0.013*\"countri\" + 0.013*\"problem\" + '\n",
      "  '0.011*\"power\" + 0.011*\"american\" + 0.011*\"america\" + 0.010*\"carbon\" + '\n",
      "  '0.009*\"energi\" + 0.009*\"pollut\" + 0.009*\"green\" + 0.008*\"fraud\" + '\n",
      "  '0.008*\"taxi\" + 0.007*\"war\" + 0.007*\"rich\"')]\n"
     ]
    }
   ],
   "source": [
    "# Load the optimal model\n",
    "import pickle\n",
    "os.chdir(r'D:\\DL_tweets_Classification\\4_Topic_modelling\\Data\\Against')\n",
    "with open('modellist_final_full.txt', 'rb') as f:\n",
    "    model_list = pickle.load(f)\n",
    "with open('cohere_final_full.txt', 'rb') as f:\n",
    "    coherence_values = pickle.load(f)\n",
    "\n",
    "optimal_model = model_list[4]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colonial-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'D:\\DL_tweets_Classification\\4_Topic_modelling\\Data\\Against')\n",
    "tweetdf = pd.read_csv(\"tweetdf_against_full.csv\")\n",
    "tweetdf.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-placement",
   "metadata": {},
   "source": [
    "# Smililar text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "descending-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_preprocessor(tweet):\n",
    "\n",
    "    tweet = tweet.replace('\\n', ' ') # remove line breaks\n",
    "    tweet = re.sub(r\"\\bhttps://t.co/\\w+\", '', tweet) # remove URL's\n",
    "    tweet = re.sub('\\w*\\d\\w*', ' ', tweet) # remove numbers      \n",
    "    \n",
    "    r = re.findall(\"@[\\w]* \", tweet) # Remove twitter handles (@users)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "        \n",
    "    r = re.findall(\"#[\\w]*\", tweet) # Remove twitter hashtages\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "        \n",
    "    r = re.findall(\"'s\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)    \n",
    "    \n",
    "    r = re.findall(\"won't\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    \n",
    "    r = re.findall(\"don't\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "        \n",
    "    r = re.findall(\"doesn't\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    \n",
    "    r = re.findall(\"didn't\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    \n",
    "    r = re.findall(\"isn't\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    \n",
    "    r = re.findall(\"'re\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "        \n",
    "    r = re.findall(\"'m\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)  \n",
    "            \n",
    "    r = re.findall(\"lol\", tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "\n",
    "    tweet = re.sub('[%s]' % re.escape(string.punctuation), '', tweet.lower()) # lower capital letters and remove punctuation \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "headed-walter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d021f487ce8449f9e007cb35981c135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_tweet = []\n",
    "for index, values in tqdm(tweetdf['Tweet'].iteritems()):\n",
    "    preprocessed = tweet_preprocessor(values)    \n",
    "    preprocessed_tweet.append(preprocessed)\n",
    "tweetdf.insert(1, column = 'preprocessed_text', value = preprocessed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affiliated-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetdf = tweetdf.reset_index()\n",
    "def tokenize(tweet):\n",
    "    for word in tweet:\n",
    "        yield(gensim.utils.simple_preprocess(str(word), deacc=True))  # deacc=True Removes punctuations\n",
    "        \n",
    "tweetdf['tidy_tweet_tokens'] = list(tokenize(tweetdf['preprocessed_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "infrared-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Stop Words\n",
    "#nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'https', 'twitter', 'twitt', 'climate change', 'global warming',\n",
    "                   'climate', 'change', 'global', 'warming', 'instagram', 'lol','good','bad',\n",
    "                   'people', 'thing', 'news', 'tweet'])\n",
    "def remove_stopwords(tweets):\n",
    "    return [[word for word in simple_preprocess(str(tweet)) if word not in stop_words] for tweet in tweets]\n",
    "tweetdf['tokens_no_stop'] = remove_stopwords(tweetdf['tidy_tweet_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "canadian-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tweet back together\n",
    "def rejoin_words(row):\n",
    "    words = row['tokens_no_stop']\n",
    "    joined_words = (\" \".join(words))\n",
    "    return joined_words\n",
    "tweetdf['no_stop_joined'] = tweetdf.apply(rejoin_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spoken-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tweetdf.no_stop_joined.values.tolist()\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Make Bigrams \n",
    "# Build the bigram model\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "frequent-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "criminal-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(tweets, allowed_postags):    \n",
    "    tweets_out = []\n",
    "    for sent in tweets:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        tweets_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return tweets_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "western-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun\n",
    "tweetdf = tweetdf.reset_index()\n",
    "tweetdf['lemmatized'] = pd.Series(lemmatization(data_words_bigrams, allowed_postags=['NOUN',\"ADJ\"]))\n",
    "tweetdf.to_pickle('lemmatized_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "former-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove very short tweets\n",
    "tweetdf['length'] = tweetdf['lemmatized'].apply(len)\n",
    "tweetdf = tweetdf.drop(tweetdf[tweetdf['length'] < 3].index)\n",
    "tweetdf = tweetdf.drop(['length'], axis=1)\n",
    "\n",
    "tweetdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "expected-accommodation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>final_location</th>\n",
       "      <th>type_local</th>\n",
       "      <th>final_lat</th>\n",
       "      <th>final_lon</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>tidy_tweet_tokens</th>\n",
       "      <th>tokens_no_stop</th>\n",
       "      <th>no_stop_joined</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>After Libs Blame West Coast Fires on Global Wa...</td>\n",
       "      <td>after libs blame west coast fires on global wa...</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>38</td>\n",
       "      <td>highlands ranch co usa</td>\n",
       "      <td>local_add</td>\n",
       "      <td>39.553877</td>\n",
       "      <td>-104.969426</td>\n",
       "      <td>21684265</td>\n",
       "      <td>[after, libs, blame, west, coast, fires, on, g...</td>\n",
       "      <td>[libs, blame, west, coast, fires, forester, sp...</td>\n",
       "      <td>libs blame west coast fires forester speaks</td>\n",
       "      <td>[libs, blame, fires_forester]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@BrainEvacuated @usagalatheart @krassenstein W...</td>\n",
       "      <td>we just a computerized bankingenergy grid glit...</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>pittsburgh pa</td>\n",
       "      <td>local_add</td>\n",
       "      <td>40.441694</td>\n",
       "      <td>-79.990086</td>\n",
       "      <td>1004116039167332354</td>\n",
       "      <td>[we, just, computerized, bankingenergy, grid, ...</td>\n",
       "      <td>[computerized, bankingenergy, grid, glitch, aw...</td>\n",
       "      <td>computerized bankingenergy grid glitch away ut...</td>\n",
       "      <td>[computerized, bankingenergy, grid, utter, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@Saintsfan5348 @FoxNews Comparing Judea-Christ...</td>\n",
       "      <td>comparing judeachristian values to sharia law...</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>new jersey usa</td>\n",
       "      <td>state_add</td>\n",
       "      <td>40.167060</td>\n",
       "      <td>-74.499870</td>\n",
       "      <td>911591139924471808</td>\n",
       "      <td>[comparing, judeachristian, values, to, sharia...</td>\n",
       "      <td>[comparing, judeachristian, values, sharia, la...</td>\n",
       "      <td>comparing judeachristian values sharia law lud...</td>\n",
       "      <td>[judeachristian, value, ludicrous, globalist, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>#GlobalWarming has created near record cold co...</td>\n",
       "      <td>has created near record cold conditions in  s...</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>38</td>\n",
       "      <td>goodyear az</td>\n",
       "      <td>local_add</td>\n",
       "      <td>33.435367</td>\n",
       "      <td>-112.357601</td>\n",
       "      <td>282220986</td>\n",
       "      <td>[has, created, near, record, cold, conditions,...</td>\n",
       "      <td>[created, near, record, cold, conditions, stra...</td>\n",
       "      <td>created near record cold conditions strange no...</td>\n",
       "      <td>[record, cold, condition, strange, excited, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>The funny part is that they are basing their a...</td>\n",
       "      <td>the funny part is that they are basing their a...</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>2</td>\n",
       "      <td>nyc ny</td>\n",
       "      <td>local_add</td>\n",
       "      <td>40.712728</td>\n",
       "      <td>-74.006015</td>\n",
       "      <td>299878544</td>\n",
       "      <td>[the, funny, part, is, that, they, are, basing...</td>\n",
       "      <td>[funny, part, basing, alarm, computer, program...</td>\n",
       "      <td>funny part basing alarm computer program compu...</td>\n",
       "      <td>[funny, part, alarm, computer, program, comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782346</th>\n",
       "      <td>1003158</td>\n",
       "      <td>1004122</td>\n",
       "      <td>Guarantee the mainstream media outlets will ne...</td>\n",
       "      <td>guarantee the mainstream media outlets will ne...</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>3793</td>\n",
       "      <td>pennsylvania usa</td>\n",
       "      <td>state_add</td>\n",
       "      <td>40.696298</td>\n",
       "      <td>-77.995133</td>\n",
       "      <td>4570518621</td>\n",
       "      <td>[guarantee, the, mainstream, media, outlets, w...</td>\n",
       "      <td>[guarantee, mainstream, media, outlets, never,...</td>\n",
       "      <td>guarantee mainstream media outlets never menti...</td>\n",
       "      <td>[guarantee_mainstream, medium, outlet, old, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782347</th>\n",
       "      <td>1003159</td>\n",
       "      <td>1004123</td>\n",
       "      <td>See, the way it's going climate change will on...</td>\n",
       "      <td>see the way it going climate change will only ...</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>268</td>\n",
       "      <td>arizona usa</td>\n",
       "      <td>state_add</td>\n",
       "      <td>34.500300</td>\n",
       "      <td>-111.500980</td>\n",
       "      <td>890058174463594496</td>\n",
       "      <td>[see, the, way, it, going, climate, change, wi...</td>\n",
       "      <td>[see, way, going, increase, desire, renewables...</td>\n",
       "      <td>see way going increase desire renewables russi...</td>\n",
       "      <td>[way, russia, national_grid, arab]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782348</th>\n",
       "      <td>1003160</td>\n",
       "      <td>1004124</td>\n",
       "      <td>Don't call this 100+ degree in late October we...</td>\n",
       "      <td>dont call this   degree in late october weathe...</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>0</td>\n",
       "      <td>los angeles ca</td>\n",
       "      <td>local_add</td>\n",
       "      <td>33.973951</td>\n",
       "      <td>-118.248405</td>\n",
       "      <td>348877502</td>\n",
       "      <td>[dont, call, this, degree, in, late, october, ...</td>\n",
       "      <td>[dont, call, degree, late, october, weather, c...</td>\n",
       "      <td>dont call degree late october weather call opp...</td>\n",
       "      <td>[degree, late, october, weather, call, opportu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782349</th>\n",
       "      <td>1003161</td>\n",
       "      <td>1004125</td>\n",
       "      <td>@SenJeffMerkley All #GOP cares about is not gi...</td>\n",
       "      <td>all  cares about is not giving another damn pe...</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>0</td>\n",
       "      <td>san diego</td>\n",
       "      <td>local_add</td>\n",
       "      <td>32.717420</td>\n",
       "      <td>-117.162773</td>\n",
       "      <td>298680916</td>\n",
       "      <td>[all, cares, about, is, not, giving, another, ...</td>\n",
       "      <td>[cares, giving, another, damn, penny, central,...</td>\n",
       "      <td>cares giving another damn penny central banker...</td>\n",
       "      <td>[damn, penny, central, banker, name]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782350</th>\n",
       "      <td>1003162</td>\n",
       "      <td>1004126</td>\n",
       "      <td>Almost 20% of Repubs believe there is no sense...</td>\n",
       "      <td>almost   of repubs believe there is no sense w...</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>10</td>\n",
       "      <td>houston tx</td>\n",
       "      <td>local_add</td>\n",
       "      <td>29.758938</td>\n",
       "      <td>-95.367697</td>\n",
       "      <td>110790607</td>\n",
       "      <td>[almost, of, repubs, believe, there, is, no, s...</td>\n",
       "      <td>[almost, repubs, believe, sense, worrying, end...</td>\n",
       "      <td>almost repubs believe sense worrying end world...</td>\n",
       "      <td>[repub, sense, end, world]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782351 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        level_0    index                                              Tweet  \\\n",
       "0             0        0  After Libs Blame West Coast Fires on Global Wa...   \n",
       "1             1        1  @BrainEvacuated @usagalatheart @krassenstein W...   \n",
       "2             2        2  @Saintsfan5348 @FoxNews Comparing Judea-Christ...   \n",
       "3             3        3  #GlobalWarming has created near record cold co...   \n",
       "4             4        4  The funny part is that they are basing their a...   \n",
       "...         ...      ...                                                ...   \n",
       "782346  1003158  1004122  Guarantee the mainstream media outlets will ne...   \n",
       "782347  1003159  1004123  See, the way it's going climate change will on...   \n",
       "782348  1003160  1004124  Don't call this 100+ degree in late October we...   \n",
       "782349  1003161  1004125  @SenJeffMerkley All #GOP cares about is not gi...   \n",
       "782350  1003162  1004126  Almost 20% of Repubs believe there is no sense...   \n",
       "\n",
       "                                        preprocessed_text    datetime  \\\n",
       "0       after libs blame west coast fires on global wa...  2018-08-13   \n",
       "1       we just a computerized bankingenergy grid glit...  2018-08-13   \n",
       "2        comparing judeachristian values to sharia law...  2018-08-13   \n",
       "3        has created near record cold conditions in  s...  2018-08-13   \n",
       "4       the funny part is that they are basing their a...  2018-08-13   \n",
       "...                                                   ...         ...   \n",
       "782346  guarantee the mainstream media outlets will ne...  2017-10-23   \n",
       "782347  see the way it going climate change will only ...  2017-10-23   \n",
       "782348  dont call this   degree in late october weathe...  2017-10-23   \n",
       "782349  all  cares about is not giving another damn pe...  2017-10-23   \n",
       "782350  almost   of repubs believe there is no sense w...  2017-10-23   \n",
       "\n",
       "        retweet_count          final_location type_local  final_lat  \\\n",
       "0                  38  highlands ranch co usa  local_add  39.553877   \n",
       "1                   0           pittsburgh pa  local_add  40.441694   \n",
       "2                   0          new jersey usa  state_add  40.167060   \n",
       "3                  38             goodyear az  local_add  33.435367   \n",
       "4                   2                  nyc ny  local_add  40.712728   \n",
       "...               ...                     ...        ...        ...   \n",
       "782346           3793        pennsylvania usa  state_add  40.696298   \n",
       "782347            268             arizona usa  state_add  34.500300   \n",
       "782348              0          los angeles ca  local_add  33.973951   \n",
       "782349              0               san diego  local_add  32.717420   \n",
       "782350             10              houston tx  local_add  29.758938   \n",
       "\n",
       "         final_lon          user_id_str  \\\n",
       "0      -104.969426             21684265   \n",
       "1       -79.990086  1004116039167332354   \n",
       "2       -74.499870   911591139924471808   \n",
       "3      -112.357601            282220986   \n",
       "4       -74.006015            299878544   \n",
       "...            ...                  ...   \n",
       "782346  -77.995133           4570518621   \n",
       "782347 -111.500980   890058174463594496   \n",
       "782348 -118.248405            348877502   \n",
       "782349 -117.162773            298680916   \n",
       "782350  -95.367697            110790607   \n",
       "\n",
       "                                        tidy_tweet_tokens  \\\n",
       "0       [after, libs, blame, west, coast, fires, on, g...   \n",
       "1       [we, just, computerized, bankingenergy, grid, ...   \n",
       "2       [comparing, judeachristian, values, to, sharia...   \n",
       "3       [has, created, near, record, cold, conditions,...   \n",
       "4       [the, funny, part, is, that, they, are, basing...   \n",
       "...                                                   ...   \n",
       "782346  [guarantee, the, mainstream, media, outlets, w...   \n",
       "782347  [see, the, way, it, going, climate, change, wi...   \n",
       "782348  [dont, call, this, degree, in, late, october, ...   \n",
       "782349  [all, cares, about, is, not, giving, another, ...   \n",
       "782350  [almost, of, repubs, believe, there, is, no, s...   \n",
       "\n",
       "                                           tokens_no_stop  \\\n",
       "0       [libs, blame, west, coast, fires, forester, sp...   \n",
       "1       [computerized, bankingenergy, grid, glitch, aw...   \n",
       "2       [comparing, judeachristian, values, sharia, la...   \n",
       "3       [created, near, record, cold, conditions, stra...   \n",
       "4       [funny, part, basing, alarm, computer, program...   \n",
       "...                                                   ...   \n",
       "782346  [guarantee, mainstream, media, outlets, never,...   \n",
       "782347  [see, way, going, increase, desire, renewables...   \n",
       "782348  [dont, call, degree, late, october, weather, c...   \n",
       "782349  [cares, giving, another, damn, penny, central,...   \n",
       "782350  [almost, repubs, believe, sense, worrying, end...   \n",
       "\n",
       "                                           no_stop_joined  \\\n",
       "0             libs blame west coast fires forester speaks   \n",
       "1       computerized bankingenergy grid glitch away ut...   \n",
       "2       comparing judeachristian values sharia law lud...   \n",
       "3       created near record cold conditions strange no...   \n",
       "4       funny part basing alarm computer program compu...   \n",
       "...                                                   ...   \n",
       "782346  guarantee mainstream media outlets never menti...   \n",
       "782347  see way going increase desire renewables russi...   \n",
       "782348  dont call degree late october weather call opp...   \n",
       "782349  cares giving another damn penny central banker...   \n",
       "782350  almost repubs believe sense worrying end world...   \n",
       "\n",
       "                                               lemmatized  \n",
       "0                           [libs, blame, fires_forester]  \n",
       "1       [computerized, bankingenergy, grid, utter, bar...  \n",
       "2       [judeachristian, value, ludicrous, globalist, ...  \n",
       "3       [record, cold, condition, strange, excited, co...  \n",
       "4       [funny, part, alarm, computer, program, comput...  \n",
       "...                                                   ...  \n",
       "782346  [guarantee_mainstream, medium, outlet, old, st...  \n",
       "782347                 [way, russia, national_grid, arab]  \n",
       "782348  [degree, late, october, weather, call, opportu...  \n",
       "782349               [damn, penny, central, banker, name]  \n",
       "782350                         [repub, sense, end, world]  \n",
       "\n",
       "[782351 rows x 15 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "environmental-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tweet back together\n",
    "def rejoin_words(row):\n",
    "    words = row['lemmatized']\n",
    "    joined_words = (\" \".join(words))\n",
    "    return joined_words\n",
    "\n",
    "tweetdf['lemmatized_joined'] = tweetdf.apply(rejoin_words, axis=1)\n",
    "tweetdf.reset_index(drop=True, inplace=True)\n",
    "stemmer = PorterStemmer()\n",
    "tweetdf['stemmed'] = tweetdf['lemmatized'].apply(lambda x : [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "perceived-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_stemmed = corpora.Dictionary(tweetdf['stemmed'])\n",
    "# Create Corpus\n",
    "tweets_stemmed = tweetdf['stemmed']\n",
    "corpus_stemmed = [id2word_stemmed.doc2bow(tweet) for tweet in tweets_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "academic-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 4\n",
    "mallet_path = 'C:\\\\mallet\\\\bin\\\\mallet'\n",
    "os.environ['MALLET_HOME'] = 'C:\\\\mallet'\n",
    "\n",
    "#ldamallet_stemmed = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_stemmed, num_topics=num_topics, id2word=id2word_stemmed)\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each documen\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rental-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, \n",
    "                                                  corpus=corpus_stemmed, \n",
    "                                                  texts=tweetdf['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fifty-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic = pd.concat([df_topic_sents_keywords, tweetdf], axis = 1)\n",
    "df_dominant_topic = df_dominant_topic[[\"Dominant_Topic\", \"Perc_Contribution\", \"Topic_Keywords\", \"Tweet\", \"datetime\", \"retweet_count\",\n",
    "                  \"final_location\", \"type_local\", \"final_lat\", \"final_lon\",\"user_id_str\"]]\n",
    "df_dominant_topic.to_csv('dominant_topic_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "maritime-fault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782351"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweetdf['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "remarkable-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>liber, polit, democrat, fire, agenda, medium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2022</td>\n",
       "      <td>year, time, earth, man, ice, human, natur, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>scienc, scientist, fact, real, datum, theori, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2644</td>\n",
       "      <td>weather, cold, day, snow, today, hot, winter, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>year, time, earth, man, ice, human, natur, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782346</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1914</td>\n",
       "      <td>liber, polit, democrat, fire, agenda, medium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782347</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>hoax, trump, gore, fake, stupid, presid, guy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782348</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>weather, cold, day, snow, today, hot, winter, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782349</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>weather, cold, day, snow, today, hot, winter, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782350</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1914</td>\n",
       "      <td>year, time, earth, man, ice, human, natur, pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782351 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic  Perc_Contribution  \\\n",
       "0                  4.0             0.2138   \n",
       "1                  1.0             0.2022   \n",
       "2                  0.0             0.1879   \n",
       "3                  2.0             0.2644   \n",
       "4                  1.0             0.2090   \n",
       "...                ...                ...   \n",
       "782346             4.0             0.1914   \n",
       "782347             3.0             0.1761   \n",
       "782348             2.0             0.2341   \n",
       "782349             2.0             0.1852   \n",
       "782350             1.0             0.1914   \n",
       "\n",
       "                                           Topic_Keywords  \n",
       "0       liber, polit, democrat, fire, agenda, medium, ...  \n",
       "1       year, time, earth, man, ice, human, natur, pla...  \n",
       "2       scienc, scientist, fact, real, datum, theori, ...  \n",
       "3       weather, cold, day, snow, today, hot, winter, ...  \n",
       "4       year, time, earth, man, ice, human, natur, pla...  \n",
       "...                                                   ...  \n",
       "782346  liber, polit, democrat, fire, agenda, medium, ...  \n",
       "782347  hoax, trump, gore, fake, stupid, presid, guy, ...  \n",
       "782348  weather, cold, day, snow, today, hot, winter, ...  \n",
       "782349  weather, cold, day, snow, today, hot, winter, ...  \n",
       "782350  year, time, earth, man, ice, human, natur, pla...  \n",
       "\n",
       "[782351 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-parish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
